{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of Encoder Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yme_TYAkaIig"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfmAIPWPtPl4",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c87eb5b-3578-4fe7-cfb7-6a6e93e6938c"
      },
      "source": [
        "# Install indicnlp library\n",
        "!pip install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/51/f4e4542a226055b73a621ad442c16ae2c913d6b497283c99cae7a9661e6c/indic_nlp_library-0.71-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Installing collected packages: morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.71 morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gUG5IJPXW96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b075ff-0723-4356-efee-0cb532354d78"
      },
      "source": [
        "# Thanks to: https://stackoverflow.com/a/48133859/14938928\n",
        "\n",
        "%%bash\n",
        "fileid=\"1TSQWZCxZIbpMjzxt4Tw2pdljoJz6ddn_\"\n",
        "curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\n",
        "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o train.csv\n",
        "\n",
        "fileid=\"1IodW8rvwGfDY52ngrd4zn5B1KkDfSM4P\"\n",
        "curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\n",
        "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o dev.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0    581      0 --:--:-- --:--:-- --:--:--   581\r100   408    0   408    0     0    581      0 --:--:-- --:--:-- --:--:--   580\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\r100 10.3M    0 10.3M    0     0  7268k      0 --:--:--  0:00:01 --:--:-- 7268k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r100   408    0   408    0     0    240      0 --:--:--  0:00:01 --:--:--   240\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "\r100 1567k  100 1567k    0     0   665k      0  0:00:02  0:00:02 --:--:--  665k\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf3eY8PmaQFY"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW3IZwF-CFLr",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc63111-f838-49bf-ad1b-c48c14568832"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "import csv\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize, indic_detokenize\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    print('⚠️⚠️⚠️ You may want to use a GPU ⚠️⚠️⚠️')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNw6DfTcCFL9"
      },
      "source": [
        "## Splitting Dataset into Train & Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gof4TpTcX8dq",
        "trusted": true
      },
      "source": [
        "# Read the dataset\n",
        "with open('train.csv', 'r') as fin_train, open('dev.csv') as fin_dev:\n",
        "    # Skip header\n",
        "    next(fin_train)\n",
        "    \n",
        "    csv_reader = csv.reader(fin_train, delimiter=',')\n",
        "    train_set = [[src_sentence, tgt_sentence] for src_sentence, tgt_sentence, _ in csv_reader]\n",
        "\n",
        "    # Skip header\n",
        "    next(fin_dev)\n",
        "    \n",
        "    csv_reader = csv.reader(fin_dev, delimiter=',')\n",
        "    dev_set = [[src_sentence, tgt_sentence] for src_sentence, tgt_sentence, _ in csv_reader]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uzOYrS-CFL-",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64315972-8859-439d-e24e-04aa34b17e10"
      },
      "source": [
        "train_size = len(train_set)\n",
        "val_size = len(dev_set)\n",
        "\n",
        "print(f'Train and Val set of {train_size} and {val_size} sentence pairs respectively')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train and Val set of 56355 and 8421 sentence pairs respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urD3XhDcCFL_",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f155665-720a-45cd-c106-e8fd87d847db"
      },
      "source": [
        "# Checking if splitting preserved integrity\n",
        "rng = random.Random()\n",
        "rng.seed(24)\n",
        "for i in range(10):\n",
        "    print(rng.choice(train_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Which Pick # has a Nationality of canada, and a Team from of sudbury wolves?', 'SELECT MIN Pick # FROM table WHERE Nationality = canada AND Team from = sudbury wolves']\n",
            "['What school did draft pick from round 3 go to?', 'SELECT  School FROM table WHERE Round = 3']\n",
            "['What is the company that made the chassis for the entrant danka arrows yamaha?', 'SELECT  Chassis FROM table WHERE Entrant = danka arrows yamaha']\n",
            "['Which driver has a Time/Retired of 2:45:46.2?', 'SELECT  Driver FROM table WHERE Time/Retired = 2:45:46.2']\n",
            "['What is the area where population in 2010 is 38062?', 'SELECT  Area (km²) FROM table WHERE Population (2010) = 38062']\n",
            "['How few runs does the 97.00 average have?', 'SELECT MIN Runs FROM table WHERE Average = 97.00']\n",
            "['List the players of the year for the tournament held in matadome ( northridge, california )?', 'SELECT  Conference Player of the Year FROM table WHERE Tournament Venue (City) = Matadome ( Northridge, California )']\n",
            "['How many schools left in 2002-03?', 'SELECT COUNT Location FROM table WHERE Left = 2002-03']\n",
            "['What is the codename for the Core i3-32xxt?', 'SELECT  Codename (main article) FROM table WHERE Brand name (list) = Core i3-32xxT']\n",
            "['What report has tour match as the status, with an against less than 22?', 'SELECT  Report FROM table WHERE Status = tour match AND Against < 22']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYm7ELZhCFMA"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrboXaDoCFMB",
        "trusted": true
      },
      "source": [
        "class Preprocessing:\n",
        "    \"\"\"\n",
        "    A class containing utitily methods for preprocessing\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "     \n",
        "    def normalize(self, sentence, lang_name):\n",
        "        \"\"\"Lowercase, tokenize a given sentence\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        sentence : str\n",
        "            Sentence to be normalized and tokenized\n",
        "        lang_name : str\n",
        "            Language name of the given sentence\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sentence: str\n",
        "            Normalized and tokenized words separted by space\n",
        "        \"\"\"\n",
        "\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = ' '.join(indic_tokenize.trivial_tokenize(sentence, lang_name))\n",
        "        return sentence\n",
        "    \n",
        "    def filter_pairs(self, pairs, max_length):\n",
        "        \"\"\"Remove pairs whose lengths is greater than max_length\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        pairs : iterable\n",
        "            Iterable object containing sentence pairs\n",
        "        max_length : int\n",
        "            Maximum length of sentences in a sentence pair\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        filtered_pairs : list\n",
        "            Filtered sentence pairs\n",
        "        \"\"\"\n",
        "\n",
        "        filter = lambda pair: len(pair[0].split(' ')) < max_length and \\\n",
        "                              len(pair[1].split(' ')) < max_length\n",
        "    \n",
        "        filtered_pairs = [pair for pair in pairs if filter(pair)]\n",
        "        return filtered_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDhWHmKhCFMD"
      },
      "source": [
        "## Building Vocabulary for Source & Target Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOC8nRieCFMD",
        "trusted": true
      },
      "source": [
        "class Language:\n",
        "    \"\"\"\n",
        "    A class to build vocabulary of source and target language\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lang_name):\n",
        "        \"\"\"Constructor to initialize the object\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        name : str\n",
        "            Name of the language\n",
        "        \"\"\"\n",
        "        self.name = lang_name\n",
        "        self.PAD, self.SOS, self.EOS, self.UNK = 0, 1, 2, 3\n",
        "        self.word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.idx2word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
        "        self.n_words = 4\n",
        "    \n",
        "    def add_sentence(self, sentence):\n",
        "        \"\"\"Populate the vocabulary with words in the sentence\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sentence : str\n",
        "            Sentence to be used to populate the vocabulary\n",
        "        \"\"\"\n",
        "\n",
        "        for word in sentence.split(' '):\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.n_words\n",
        "                self.idx2word[self.n_words] = word\n",
        "                self.n_words += 1\n",
        "            \n",
        "    def sentence_to_indices(self, sentence):\n",
        "        \"\"\"Converts a sentence to sequence of index\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        sentence : str\n",
        "            Sentence for which sequence of index is to be calculated\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        indices : list\n",
        "            Sequence of index appended by EOS token index\n",
        "        \"\"\"\n",
        "        \n",
        "        indices = [self.word2idx[word] if word in self.word2idx else self.UNK\n",
        "                   for word in sentence.split(' ')]\n",
        "        indices.append(self.EOS)\n",
        "    \n",
        "        return indices\n",
        "    \n",
        "    def indices_to_sentence(self, indices):\n",
        "        \"\"\"Converts sequence of index to corresponding sentence\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indices : iterable\n",
        "            Sequence of index\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sentence : str\n",
        "            Sentence corresponding to given sequence of index\n",
        "        \"\"\"\n",
        "\n",
        "        sentence = ' '.join(self.idx2word[index] for index in indices)\n",
        "        return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ypjSBXaY24"
      },
      "source": [
        "# Preprocessing object\n",
        "preprocess = Preprocessing()\n",
        "\n",
        "# Source and target language\n",
        "src_lang, tgt_lang = Language('en'), Language('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QOVR_RjCFMB",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1f4324-dc48-4577-b733-2d06de67b15c"
      },
      "source": [
        "# Normalize the training set sentence pairs\n",
        "train_sentences = [\n",
        "    [preprocess.normalize(src_sentence, src_lang.name), preprocess.normalize(tgt_sentence, tgt_lang.name)]\n",
        "    for src_sentence, tgt_sentence in train_set\n",
        "]\n",
        "print(f'Normalized {len(train_sentences)} sentence pairs')\n",
        "\n",
        "# Filter the training set sentence pairs\n",
        "train_sentences = preprocess.filter_pairs(train_sentences, max_length=36)\n",
        "print(f'Filtered to {len(train_sentences)} sentence pairs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized 56355 sentence pairs\n",
            "Filtered to 56172 sentence pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGgWIINYCFME",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c174c377-3eaf-41a1-dc07-7feaafcb0f55"
      },
      "source": [
        "# Add training set sentences to their corresponding language\n",
        "for src_sentence, tgt_sentence in train_sentences:\n",
        "    src_lang.add_sentence(src_sentence)\n",
        "    tgt_lang.add_sentence(tgt_sentence)\n",
        "\n",
        "print(f'No. of words in source language: {src_lang.n_words}')\n",
        "print(f'No. of words in target language: {tgt_lang.n_words}')\n",
        "\n",
        "print(random.choice(train_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of words in source language: 38137\n",
            "No. of words in target language: 35872\n",
            "['how many records were made on the game that ended with score w 121–119 ( ot )', 'select count record from table where score = w 121–119 ( ot )']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwnfjQ4yCFMF"
      },
      "source": [
        "## Convert sentences\n",
        "Converting senteces to integer sequences according to their vocabulary and then transforming them into tensors for input to Pytorch Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdLE_x7_CFMG",
        "trusted": true
      },
      "source": [
        "# Convert training set sentences to corresponding sequence of index\n",
        "train_indices = [\n",
        "    [src_lang.sentence_to_indices(src_sentence), tgt_lang.sentence_to_indices(tgt_sentence)]\n",
        "    for src_sentence, tgt_sentence in train_sentences\n",
        "]\n",
        "\n",
        "assert len(train_indices) == len(train_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLnhAz6DCFMH",
        "trusted": true
      },
      "source": [
        "def to_tensor(indices):\n",
        "    \"\"\"Converts sequence of index to tensors\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    indices : iterable\n",
        "        Sequence of index\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    Pytorch tensor of corresponding sequence of index\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long, device=device)\n",
        "\n",
        "# Convert training set sequences of index to tensors\n",
        "train_tensors = [\n",
        "    [to_tensor(src_indices), to_tensor(tgt_indices)] for src_indices, tgt_indices in train_indices\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkRsPFYBtPmL",
        "trusted": true
      },
      "source": [
        "def collate(batch):\n",
        "    \"\"\"Utitlity function for batching via DataLoader\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : iterable\n",
        "        Batch of tensor pairs\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    padded_source_tensors : 2-D tensor (batch_size, seq_len)\n",
        "        Source language tensors padded by source language PAD token\n",
        "    padded_target_tensors : 2-D tensor (batch_size, seq_len)\n",
        "        Source language tensors padded by source language PAD token\n",
        "    \"\"\"\n",
        "    \n",
        "    src_tensors, tgt_tensors = zip(*batch)\n",
        "\n",
        "    padded_src_tensors = pad_sequence(src_tensors, padding_value=src_lang.PAD, batch_first=True)\n",
        "    padded_tgt_tensors = pad_sequence(tgt_tensors, padding_value=tgt_lang.PAD, batch_first=True)\n",
        "\n",
        "    return padded_src_tensors, padded_tgt_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm-3cfcoCFMH"
      },
      "source": [
        "## Seq2Seq Model using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0QeGJjsCFMI",
        "trusted": true
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A class implementing Bi-GRU Encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        \"\"\"Constructor to initialize the Encoder object\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            Vocabulary size of source language\n",
        "        embedding_dim : int\n",
        "            Size of embedding vectors of words in source language\n",
        "        hidden_size : int\n",
        "            Size of hidden state vectors of Encoder\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        self.W1 = nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(2*hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"Implements the forward pass of Encoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input : 2-D tensor (batch_size, seq_len)\n",
        "            Source tensors\n",
        "        hidden : 3-D tensor (num_layers, batch_size, hidden_size)\n",
        "            Hidden state vector\n",
        "                \n",
        "        Returns\n",
        "        -------\n",
        "        output : 3-D tensor (batch_size, seq_len, hidden_size)\n",
        "            Encoder hidden states of all timesteps\n",
        "        hidden : 3-D tensor (1, batch_size, hidden_size)\n",
        "            Hidden state vector of last timestep\n",
        "        \"\"\"\n",
        "        \n",
        "        embedding = self.embedding(input)\n",
        "        \n",
        "        output, hidden = self.gru(embedding, hidden)\n",
        "        output = self.W1(output)\n",
        "        \n",
        "        hidden = torch.cat((hidden[0, :, :], hidden[1, :, :]), dim=1).unsqueeze(0)\n",
        "        hidden = self.W2(hidden)\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state vector for Bi-GRU Encoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            Batch size\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Tensor initialized with all zeroes of shape (2, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        \n",
        "        return torch.zeros(2, batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCu_mG3dCFMJ",
        "trusted": true
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A class implementing Bahdanau Attention Decoder with GRU units\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        \"\"\"Constructor to initialize the Decoder object\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            Vocabulary size of target language\n",
        "        embedding_dim : int\n",
        "            Size of embedding vectors of words in target language\n",
        "        hidden_size : int\n",
        "            Size of hidden state vectors of Decoder\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "        self.gru = nn.GRU(embedding_dim + hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        \"\"\"Implements the forward pass of Decoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input : 2-D tensor (batch_size, seq_len)\n",
        "            Target tensors\n",
        "        hidden : 3-D tensor (1, batch_size, hidden_size)\n",
        "            Hidden state vector\n",
        "        encoder_outputs : 3-D tensor (batch_size, seq_len, hidden_size)\n",
        "            Output of each timestep of Encoder\n",
        "                \n",
        "        Returns\n",
        "        -------\n",
        "        output : 3-D tensor (batch_size, seq_len, hidden_size)\n",
        "            Decoder output of seq_len timesteps\n",
        "        hidden : 3-D tensor (1, batch_size, hidden_size)\n",
        "            Decoder hidden state vector\n",
        "        \"\"\"\n",
        "        \n",
        "        embedding = self.embedding(input)\n",
        "        \n",
        "        tmp_hidden = hidden.permute(1, 0, 2)\n",
        "        scores = torch.tanh(self.W1(tmp_hidden) + self.W2(encoder_outputs))\n",
        "        attn_weights = F.softmax(self.V(scores), dim=1)\n",
        "        \n",
        "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1, keepdim=True)\n",
        "        output = torch.cat((embedding, context_vector), -1)\n",
        "        \n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state vector\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            Batch size\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Tensor initialized with all zeroes of shape (1, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gan2KV19CFMK",
        "trusted": true
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    A class implementing end-to-end seq2seq model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, hidden_size, embedding_dim, SOS, EOS):\n",
        "        \"\"\"Construcor to initialize seq2seq object\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        src_vocab_size : int\n",
        "            Source vocabulary size\n",
        "        tgt_vocab_szie : int\n",
        "            Target vocabulary size\n",
        "        hidden_size : int\n",
        "            Hidden state vector size same for both Encoder and Decoder\n",
        "        embedding_dim : int\n",
        "            Embedding dimension vector size same for both Encoder and Decoder\n",
        "        SOS : int\n",
        "            Index value of Start of Sentence token\n",
        "        EOS : int\n",
        "            Index value of End of Sentence token\n",
        "        \"\"\"\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=src_vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_size=hidden_size\n",
        "        ).to(device)\n",
        "        \n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_size=hidden_size\n",
        "        ).to(device)\n",
        "        \n",
        "        self.SOS = SOS\n",
        "        self.EOS = EOS\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state vectors of Encoder and Decoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            Batch size\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Encoder and Decoder initial hidden state tensor\n",
        "        \"\"\"\n",
        "        \n",
        "        return self.encoder.init_hidden(batch_size), self.decoder.init_hidden(batch_size)\n",
        "        \n",
        "    def forward(self, src_tensors, tgt_tensors, criterion, tf):\n",
        "        \"\"\"Implements forward pass of end-to-end seq2seq2 model\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        src_tensors : 2-D tensor (batch_size, seq_len)\n",
        "            Source tensors\n",
        "        tgt_tensors : 2-D tensor (batch_size, seq_len)\n",
        "            Target tensors\n",
        "        criterion\n",
        "            loss function\n",
        "                \n",
        "        Returns\n",
        "        -------\n",
        "        Trainig loss incured\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = src_tensors.size(0)\n",
        "        encoder_hidden, decoder_hidden = self.init_hidden(batch_size)\n",
        "        \n",
        "        encoder_outputs, encoder_hidden = self.encoder(src_tensors, encoder_hidden)\n",
        "        \n",
        "        decoder_input = torch.full((batch_size, 1), self.SOS, dtype=torch.long, device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        loss = 0\n",
        "        \n",
        "        if random.random() < tf:\n",
        "            for timestep in range(tgt_tensors.size(1)):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input,\n",
        "                                                              decoder_hidden,\n",
        "                                                              encoder_outputs)\n",
        "                # Teacher forcing\n",
        "                decoder_input = tgt_tensors[:, timestep].unsqueeze(1)\n",
        "                loss += criterion(decoder_output.squeeze(dim=1), tgt_tensors[:, timestep])\n",
        "        else:\n",
        "            for timestep in range(tgt_tensors.size(1)):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input,\n",
        "                                                              decoder_hidden,\n",
        "                                                              encoder_outputs)\n",
        "                _, topi = torch.topk(decoder_output, 1)\n",
        "                \n",
        "                decoder_input = topi.view(batch_size, 1)\n",
        "                loss += criterion(decoder_output.squeeze(dim=1), tgt_tensors[:, timestep])\n",
        "\n",
        "        return loss / tgt_tensors.size(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NJAtLltCFML",
        "trusted": true
      },
      "source": [
        "def train(model, train_loader, epochs, lr, tf):\n",
        "    \"\"\"Implements training of given seq2seq model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : Seq2Seq\n",
        "        A seq2seq model\n",
        "    train_loader : DataLoader\n",
        "        DataLoader for getting batches\n",
        "    epochs : int\n",
        "        Number of epochs\n",
        "    lr : float\n",
        "        Learning rate\n",
        "    \"\"\"\n",
        "    \n",
        "    for parameter in model.parameters():\n",
        "        if parameter.dim() > 1:\n",
        "            nn.init.xavier_uniform_(parameter)\n",
        "    \n",
        "    optimizer = optim.Adam([parameter for parameter in model.parameters()], lr=lr)\n",
        "    criterion = nn.NLLLoss()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = no_of_batches = 0\n",
        "        \n",
        "        with tqdm(train_loader, unit='batch') as tr:            \n",
        "            for src_tensors, tgt_tensors in tr:\n",
        "                tr.set_description(f'Epoch {(epoch + 1):>2}')\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "                loss = model(src_tensors, tgt_tensors, criterion, tf)\n",
        "            \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "                no_of_batches += 1\n",
        "            \n",
        "                tr.set_postfix(loss=f'{epoch_loss / no_of_batches:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpxuSrzmtPmV",
        "trusted": true
      },
      "source": [
        "def predict(model, src_tensor, max_length):\n",
        "    \"\"\"Implements prediction for a given source tensor\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : Seq2Seq\n",
        "        Trained seq2seq model\n",
        "    src_tensor: 2-D tensor\n",
        "        Source tensor of shape (1, seq_len)\n",
        "    max_length: int\n",
        "        Maximum length of predicted target sentence\n",
        "            \n",
        "    Returns\n",
        "    -------\n",
        "    prediction : list\n",
        "        Sequence of target index\n",
        "    \"\"\"\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, decoder_hidden = model.init_hidden(1)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = model.encoder(src_tensor, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[tgt_lang.SOS]], dtype=torch.long, device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        prediction = []\n",
        "\n",
        "        for timestep in range(max_length):\n",
        "            decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            _, topi = torch.topk(decoder_output, 1)\n",
        "\n",
        "            if topi.item() == tgt_lang.EOS:\n",
        "                break\n",
        "            else:\n",
        "                prediction.append(topi.item())\n",
        "\n",
        "            decoder_input = torch.tensor([[topi.item()]], dtype=torch.long, device=device)\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbvlQy5vtPmT",
        "trusted": true
      },
      "source": [
        "# Hyperparameters of seq2seq model\n",
        "embedding_dim = 512\n",
        "hidden_size = 512\n",
        "batch_size = 64\n",
        "epochs = 11\n",
        "lr = 0.001\n",
        "tf = 1\n",
        "\n",
        "# Seq2Seq model object\n",
        "model = Seq2Seq(\n",
        "    src_vocab_size=src_lang.n_words,\n",
        "    tgt_vocab_size=tgt_lang.n_words,\n",
        "    hidden_size=hidden_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    SOS=tgt_lang.SOS,\n",
        "    EOS=tgt_lang.EOS\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dak5wIU8aidX"
      },
      "source": [
        "## Training\n",
        "\n",
        "Skip this section if training is not to be done, i.e., only inferencing purposes model is to be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ueydDQCFMM",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad4b17b-2236-41fe-8f82-09beb9309f21"
      },
      "source": [
        "train_loader = DataLoader(train_tensors, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
        "\n",
        "# Training the model\n",
        "model.train()\n",
        "train(model=model, train_loader=train_loader, epochs=epochs, lr=lr, tf=tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1: 100%|██████████| 878/878 [07:04<00:00,  2.07batch/s, loss=1.9763]\n",
            "Epoch  2: 100%|██████████| 878/878 [07:06<00:00,  2.06batch/s, loss=0.9917]\n",
            "Epoch  3: 100%|██████████| 878/878 [07:05<00:00,  2.06batch/s, loss=0.5471]\n",
            "Epoch  4: 100%|██████████| 878/878 [07:05<00:00,  2.07batch/s, loss=0.3066]\n",
            "Epoch  5: 100%|██████████| 878/878 [07:05<00:00,  2.06batch/s, loss=0.1906]\n",
            "Epoch  6: 100%|██████████| 878/878 [07:05<00:00,  2.07batch/s, loss=0.1438]\n",
            "Epoch  7: 100%|██████████| 878/878 [07:04<00:00,  2.07batch/s, loss=0.1187]\n",
            "Epoch  8: 100%|██████████| 878/878 [07:04<00:00,  2.07batch/s, loss=0.0984]\n",
            "Epoch  9: 100%|██████████| 878/878 [07:03<00:00,  2.07batch/s, loss=0.0931]\n",
            "Epoch 10: 100%|██████████| 878/878 [07:05<00:00,  2.06batch/s, loss=0.0830]\n",
            "Epoch 11: 100%|██████████| 878/878 [07:07<00:00,  2.06batch/s, loss=0.0688]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdVufz-9RA7r",
        "trusted": true
      },
      "source": [
        "torch.save(model.state_dict(), 'seq2seq_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-2ks3OXCFMN"
      },
      "source": [
        "## Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Geu06ja6ia"
      },
      "source": [
        "model.load_state_dict(torch.load('seq2seq_model.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ujZ784CFMO",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f608bca-e8b5-488b-dad0-a9e97d11ff44"
      },
      "source": [
        "# Validation set sentences\n",
        "src_sentences, tgt_sentences = zip(*dev_set)\n",
        "\n",
        "# Preprocess validation set source sentences\n",
        "preprocess = Preprocessing()\n",
        "src_sentences = [preprocess.normalize(src_sentence, src_lang.name) for src_sentence in src_sentences]\n",
        "print(f'Normalized {len(src_sentences)} source sentences')\n",
        "\n",
        "# Validation set source tensors\n",
        "src_tensors = [to_tensor(src_lang.sentence_to_indices(src_sentence)) for src_sentence in src_sentences]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized 8421 source sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZj9ecVZCFMP",
        "trusted": true
      },
      "source": [
        "def predictions(model, src_loader, tgt_lang):\n",
        "    \"\"\"\n",
        "    Validation or test set predictions\n",
        "    \"\"\"\n",
        "    tgt_predictions = []\n",
        "    \n",
        "    with tqdm(src_loader, unit='sentences') as tr:            \n",
        "        for src_tensor, _ in tr:\n",
        "            tgt_indices = predict(model, src_tensor, 36)\n",
        "            tgt_prediction = tgt_lang.indices_to_sentence(tgt_indices)\n",
        "            tgt_predictions.append(tgt_prediction)\n",
        "        \n",
        "    return tgt_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEbVkYfQX8eD",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea266b6-2205-4fc1-a605-185707c393b4"
      },
      "source": [
        "val_loader = DataLoader(src_tensors, batch_size=1)\n",
        "tgt_predictions = predictions(model, val_loader, tgt_lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8421/8421 [03:10<00:00, 44.21sentences/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cghT7M4CFMQ",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ef34ec-7102-453c-ac50-e7cf8d5d667b"
      },
      "source": [
        "rng = random.Random()\n",
        "rng.seed(42)\n",
        "indices = [rng.randrange(0, len(dev_set)) for i in range(5)]\n",
        "\n",
        "for idx in indices:\n",
        "    print(f'> {src_sentences[idx]}')\n",
        "    print(f'= {tgt_sentences[idx]}')\n",
        "    print(f'< {tgt_predictions[idx]}')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> how many pole positions for round 20 ?\n",
            "= SELECT COUNT Pole Position FROM table WHERE Round = 20\n",
            "< select count pole position from table where round = 20\n",
            "\n",
            "> what is the pa when the skip is colleen jones ?\n",
            "= SELECT  PA FROM table WHERE Skip = Colleen Jones\n",
            "< select entities from table where skip = erkki jones\n",
            "\n",
            "> what is the kosal with hatibandha as the sambalpuri cinema ?\n",
            "= SELECT  Kosal FROM table WHERE Sambalpuri Cinema = hatibandha\n",
            "< select detriment from table where 07:00 = yak\n",
            "\n",
            "> in the game on or before week 9 , who was the opponent when the attendance was 61,626 ?\n",
            "= SELECT  Opponent FROM table WHERE Week < 9 AND Attendance = 61,626\n",
            "< select opponent from table where week < 9 and attendance = 914\n",
            "\n",
            "> which competition has a venue of estadio alfonso lastras , san luis potosí , mexico , and a goal larger than 15 ?\n",
            "= SELECT  Competition FROM table WHERE Venue = estadio alfonso lastras, san luis potosí, mexico AND Goal > 15\n",
            "< select sum competition from table where venue = estadio ciudad , san salvador and venue = thessaloniki , syria and goal > 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9VzHdpUsYVP",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0271b1bc-835c-45a2-cb30-c2ea10f2e4f4"
      },
      "source": [
        "!pip install \"nltk==3.4.5\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449910 sha256=06cd45584681423f12e912f38f6897da0f1a8bb2d27adf51aab89175f791d6d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLMVHvJ4CFMR",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07429243-6414-46c1-c3e4-879c8b82befa"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "def print_scores(true_sentences, pred_sentences):\n",
        "    if len(true_sentences) != len(pred_sentences):\n",
        "        print(f'E: Number of sentences do not match. True: {len(true_sentences)} Pred: {len(pred_sentences)}')\n",
        "        return\n",
        "\n",
        "    for i in range(len(true_sentences)):\n",
        "        true_sentences[i] = true_sentences[i].lower()\n",
        "        pred_sentences[i] = pred_sentences[i].lower()\n",
        "    \n",
        "    true_sentences_joined, pred_sentences_joined = [], []\n",
        "\n",
        "    for i in range(len(true_sentences)):\n",
        "        # some punctuations from string.punctuation\n",
        "        split_true = list(filter(None, re.split(r'[\\s!\"#$%&\\()+,-./:;<=>?@\\\\^_`{|}~]+', true_sentences[i])))\n",
        "        split_pred = list(filter(None, re.split(r'[\\s!\"#$%&\\()+,-./:;<=>?@\\\\^_`{|}~]+', pred_sentences[i])))\n",
        "\n",
        "        true_sentences_joined.append(' '.join(split_true))\n",
        "        pred_sentences_joined.append(' '.join(split_pred))\n",
        "\n",
        "    print(f'Number of sentences: {len(true_sentences_joined)}')\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    # Macro-averaged BLEU-4 score.\n",
        "    scores['bleu_4_macro'] = 0\n",
        "    for ref, hyp in zip(true_sentences_joined, pred_sentences_joined):\n",
        "        scores['bleu_4_macro'] += sentence_bleu(\n",
        "            [ref.split()],\n",
        "            hyp.split(),\n",
        "            smoothing_function=SmoothingFunction().method2\n",
        "        )\n",
        "    scores['bleu_4_macro'] /= len(true_sentences_joined)\n",
        "\n",
        "    # BLEU-4 score.\n",
        "    scores['bleu_4'] = corpus_bleu(\n",
        "        [[ref.split()] for ref in true_sentences_joined],\n",
        "        [hyp.split() for hyp in pred_sentences_joined],\n",
        "        smoothing_function=SmoothingFunction().method2\n",
        "    )\n",
        "\n",
        "    # METEOR score.\n",
        "    scores['meteor'] = 0\n",
        "    for ref, hyp in zip(true_sentences_joined, pred_sentences_joined):\n",
        "        scores['meteor'] += single_meteor_score(ref, hyp)\n",
        "    scores['meteor'] /= len(true_sentences_joined)\n",
        "\n",
        "    # Print out scores.\n",
        "    for key in scores:\n",
        "        print(f'{key}: {scores[key]}')\n",
        "\n",
        "print_scores(list(tgt_sentences), tgt_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 8421\n",
            "bleu_4_macro: 0.6424026061807562\n",
            "bleu_4: 0.5977420791225594\n",
            "meteor: 0.7844590430266942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8P68TaACFMT"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2URQHjsNX8eG",
        "trusted": true
      },
      "source": [
        "# Test set source sentences\n",
        "with open('testhindistatements.csv', 'r') as fin:\n",
        "    # Skip header\n",
        "    next(fin)\n",
        "    \n",
        "    csv_reader = csv.reader(fin, delimiter=',')\n",
        "    src_sentences = [src_sentence for _, _, src_sentence in csv_reader]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJ4sBKTX8eH",
        "trusted": true,
        "outputId": "5fbf194b-027b-4cfe-d84d-640e03ba5d6f"
      },
      "source": [
        "# Preprocess test set source sentences\n",
        "preprocess = Preprocessing()\n",
        "src_sentences = [preprocess.normalize(src_sentence, src_lang.name) for src_sentence in src_sentences]\n",
        "print(f'Normalized {len(src_sentences)} source sentences')\n",
        "\n",
        "# Test set source tensors\n",
        "src_tensors = [to_tensor(src_lang.sentence_to_indices(src_sentence)) for src_sentence in src_sentences]\n",
        "\n",
        "assert len(src_sentences) == len(src_tensors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized 24102 source sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TO7oA9DX8eH",
        "trusted": true,
        "outputId": "30ce3076-5581-4b14-ea4d-3278b794b5b3"
      },
      "source": [
        "test_loader = DataLoader(src_tensors, batch_size=1)\n",
        "tgt_predictions = predictions(model, test_loader, tgt_lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 24102/24102 [05:08<00:00, 78.22sentences/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU_3wcjdX8eI",
        "trusted": true,
        "outputId": "42ed681e-1e7d-4715-ba12-aa01669f5376"
      },
      "source": [
        "# Generate answer.txt\n",
        "with open('answer.txt', 'w') as fout:\n",
        "    for tgt_prediction in tqdm(tgt_predictions):\n",
        "        fout.write(f'{tgt_prediction}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 24102/24102 [00:00<00:00, 943656.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMMwGJywPoO"
      },
      "source": [
        "**References**\n",
        "- Neural Machine Translation by Jointly Learning to Align and Translate  \n",
        "[arXiv:1409.0473](https://arxiv.org/abs/1409.0473)"
      ]
    }
  ]
}